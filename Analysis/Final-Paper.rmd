---
title: "Final Paper"
author: "STOR320.002 Group 13"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(glmnet)
library(caret)
library(leaps)
library(gridExtra)

studalc <- read.csv("https://raw.githubusercontent.com/kwonjhk/StudentAlcohol/master/Data/studalc-por.csv") %>%
  mutate(Gedu = factor(Gedu, 
    levels = c("None", "Primary", "Middle", "Secondary", "Higher"),
    labels = c("Minimal", "Minimal", "Middle", 
               "Secondary", "Higher")), 
    freetime = factor(freetime, c("Very Low", "Low", "Medium", "High", "Very High")), 
    famrel = factor(famrel, c("Very Bad", "Bad", "Neutral", "Good", "Excellent")), 
    G4 = as.factor(G4))

set.seed(1)
test = sample(1:nrow(studalc), 100)

```

This project was a collaboration with the following group members: Jinhyun Kwon, Drew Robinson, Clement Li, Xiangman Zhao, and Matthew Donnalley.

# INTRODUCTION

Grades and test scores—these are, in nearly all contemporary academic settings, the sole representations of student success. Student success, in turn, exposes the strengths and shortcomings of an education system as a whole. Hence, politicians, administrators and teachers all have at the very least marginal interest in optimizing their education systems to continually improve school performance, and they take action through a variety of methods: new teaching methods, academic research, increased funding, additional educational resources, etc. Robust statistical analysis has become an essential tool in addressing this challenge, specifically in identifying what makes a good student “good”; the best way to improve grades overall is to observe the commonalities among top students and promote those patterns throughout the entire system.


We seek to provide our own contribution in answering such questions. The focus of our analysis is to delve into the many relationships between student information and school performance and extract those characteristics that seem to have the greatest impact on a student’s performance in school. The data used in our statistical analysis is composed of a miscellany of student information, including term grades, originally collected through a survey of high school students in Portugal.


The information recorded in our data can naturally be divided into several groupings, two of which are the behavioral and background group. A comparison of these two variable groups is of particular interest because it addresses the question of whether a student can overcome personal disadvantage and achieve success in school through diligence and prudence, or whether a student afforded certain advantages by personal or family background can succeed in spite of poor personal decision-making. This widespread perception that students are unfairly disadvantaged by socioeconomic status was the primary motivation behind the adoption of the (somewhat controversial) policy of affirmative action by numerous universities in the United States. Therefore, our first question asks: which has more influence on grade improvement from the first term to the second term: background variables or behavioral variables? With this question, we hope to discern meaningful associations between grade improvement and the variables of the two groups and subsequently compare the strength of those associations with some predictive modeling. If behavioral variables show greater predictive value, educational reformers may want to target student choices and motivations to improve grades. On the other hand, if environmental factors are better predictors, schools may want to give extra attention to students of certain demographic groups.


Our second question looks at another set of information: educational variables, variables that directly relate to a student’s educational motivations, resources, and history. Specifically, our question is: which educational variables are most predictive of a student’s final term grade? We want to identify the educational variables from our dataset that most relate to a student’s performance in school. For instance, if students who have one or several failing grades in their academic record are always expected to be the bottom-scoring students, schools may want to divert extra attention to those students. As another example, perhaps taking extra classes or receiving supplementary tutoring does not significantly help students become a top scorer and is thus not really worth the money.




# DATA


This dataset was originally collected by researchers at the University of Minho in Braga, Portugal; Paulo Cortez and Alice Silva of the University of Minho analyzed this specific data in an academic paper titled “Using Data Mining to Predict Secondary School Student Performance” published through EUROSIS [ISBN 978-9077381-39-7]. The data is a sample of high school students obtained through a survey of students taking Math and Portuguese language courses at two high schools in Portugal: Gabriel Pereira and Mousinho da Silveira. Each observation represents the survey response of a single student; the variables gathered from the survey contain a wide breadth of information about the students and their families — in total, there are 33 variables. To avoid issues like duplicate students and differences in grades due to overall course difficulty (Math vs Portuguese), our analysis focuses only on the data containing survey responses of students in the Portuguese language class. The Portuguese language class dataset is composed of 649 unique student survey responses.


We split the variables into four categories: response, behavioral, background, and educational. The response variables are G4 and G3. G4 is a binary variable we created based on the given variables G1 and G2, which are the grades for the first and second term; G4 is 1 if G2 is greater than or equal to G1 (indicating improvement from the first term grade) and 0 otherwise. G3 is a numeric variable representing the final grade of a student, given in points out of 20. G1 and G2, mentioned earlier, use the same units as G3.


```{r echo = FALSE}
studalc %>%
  mutate(G4 = ifelse(G4 == 1, "Increased", "Decreased")) %>%
  ggplot(aes(x = G1, y = G2, colour = G4)) + 
  geom_point(position = position_jitter(.3, .3)) + 
  geom_abline(aes(slope = 1, intercept = 0), linetype = "dashed", 
              colour = "grey", size = 2) + 
  ylim(-1, 20) + xlim(-1, 20)
```


The behavioral variables are Alc, goout, freetime, romantic, activities, and health. Alc, defined as 1 if the student drinks and 0 if the student does not drink, is a binary variable we created using two other variables, Walc and Dalc (weekend and weekday alcohol consumption). The factor variable goout describes the amount of time a student spends going out with friends (“Very Low” to “Very High”). Freetime represents the amount of free time a student has after school (also “Very Low” to “Very High”). Romantic is a binary variable indicating whether a student is in a romantic relationship (1) or not (0). Similarly, activities is a binary variable that is 1 if a student participates in extracurricular activities. Finally, health is a factor variable with 5 levels that describes the overall health status of a student (“Very Bad” to “Very Good”).


The background variables are sex, address, Mjob, Pjob, nursery, Gedu, famsize, internet, and traveltime. Sex is a binary variable with value “F” for female and “M” for male. Address is also a binary variable indicating whether a student lives in an “Urban” or “Rural” area. Mjob and Fjob are factor variables representing the occupation of a student’s mother and father, respectively (“teacher,” “health” related, civil “services,” “at_home,” and “other”). Nursery is a binary variable that is 1 if the student attended nursery school as a child and 0 otherwise. Internet represents whether a student does (1) or does not (0) have home Internet access. Gedu is a categorical metric we created based on three other variables, guardian, Medu, and Fedu, that denotes the highest completed education level of a student’s guardian (“Minimal” education, “Middle” school, “Secondary” school, and “Higher” education). Traveltime is a numeric variable representing the time, in hours, a student spends going from home to school.


The educational variables are schoolsup, famsup, paid, absences, studytime, failures, firstgen, and reason. Schoolsup and famsup are binary variables indicating whether a student receives school and family educational support, respectively. Paid is another binary variable that reveals whether a student takes paid classes outside of school. Absences is a discrete numeric variable denoting the number of school absences. Failures is also a discrete numeric variable counting the number of past class failures. Reason describes the reason why a student chose to attend their high school: proximity to “home,” school “reputation,” “course” preference, and “other.” Firstgen is a binary variable we made using Medu, Fedu, and higher (a variable representing whether a student wants to pursue higher education); firstgen indicates whether a student is an aspiring first-generation college student, defined as a student who wants to go to college (based on higher) and whose parents both never completed secondary school. We do not have enough information to confirm whether these students match the traditional definition of first-generation but named the variable firstgen merely out of convenience.


```{r echo = FALSE}
binary.var = c("Alc", "romantic", "activities", "sex", "address", "nursery", "internet", "schoolsup", "famsup", "paid", "firstgen")
numeric.var = c("traveltime", "absences", "studytime", "failures")
factor.var = c("goout", "freetime", "health", "Mjob", "Fjob", "Gedu", "reason")

data.frame(Behavioral = c("Alc",
                        "romantic", 
                        "activities",
                        "goout", 
                        "freetime", 
                        "health", 
                        "", 
                        ""), 
           Background = c("sex",
                    "address",
                    "nursery",
                    "internet", 
                    "Mjob", 
                    "Fjob",
                    "Gedu",
                    "traveltime"),
           Educational = c("schoolsup", 
                           "famsup", 
                           "paid", 
                           "firstgen",
                           "reason",
                           "absences", 
                           "studytime", 
                           "failures")) %>%
  mutate_all(~cell_spec(.x, color = ifelse(.x %in% binary.var, "red", ifelse(.x %in% numeric.var, "blue", "green")))) %>%
  knitr::kable("html", caption = "Variable Groups", escape = F) %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  add_header_above(c("Question 1" = 2, "Question 2" = 1)) %>%
  footnote(general = c("Binary = Red", "Categorical = Green", "Numeric = Blue"))
```


# RESULTS

Our first analysis addresses the question: which set of variables, the behavioral or background variables, can better predict whether a student improves his/her grade from the first term to the second term? As mentioned in the Data section, we created a binary variable, G4, that is 1 if G2-G1 is positive or zero and 0 if G2-G1 is negative; G4 represents the response variable for this first question. The general idea behind our analysis is to create different classification models using the two sets of variables, behavioral and background. The models we used were: standard logistic regression, logistic regression with all two-way interactions, KNN, and random forests. Logistic regression and KNN cannot handle categorical variables (which make up the vast majority of our data) natively. Random forest models, however, can handle categorical variables natively because they are an aggregate of a large number of decision trees, which can split on categorical variables easily.


Prior to any model training, the data was split into a training and test set randomly; the training set consisted of 549 observations, and the test set contained the remaining 100 observations. The logistic regression models were fit without any variable selection. The KNN models were tuned using 10-fold cross-validation to find the optimal value for K. For the random forest models, the optimal number of variables in each decision tree was selected using out-of-bag error. The models were tuned and trained on the training set, and their performances were evaluated on the test set; test error was measured as the misclassification rate, the rate at which a model incorrectly classifies an observation.


```{r echo = FALSE, warning = FALSE}
glm1 = glm(G4~Alc+goout + freetime +romantic +activities +health, family = binomial, data = studalc[-test,])

glm2 = glm(G4~sex + address + Mjob + Fjob + nursery + Gedu + internet + traveltime, family = binomial, data = studalc[-test,])

glm3 = glm(G4~(Alc + goout + freetime + romantic + activities + health)^2, family = binomial, data = studalc[-test,])

glm4 = glm(G4~(sex + address + Mjob + Fjob + nursery + Gedu +internet +traveltime)^2, family = binomial, data= studalc[-test,])



grid = expand.grid(k = 1:50)
ctrl = trainControl(method='cv', number = 10, savePred = FALSE)

set.seed(3)
knn.bg = train(G4 ~ sex + address + Mjob + Fjob + nursery + Gedu + 
               internet + traveltime, 
             studalc[-test,], method = "knn", 
             preProc = c("center", "scale"),
             trControl = ctrl, tuneGrid = grid)

knn.bv = train(G4 ~ Alc + goout + freetime + romantic + activities + 
               health, 
             studalc[-test,], method = "knn", 
             preProc = c("center", "scale"),
             trControl = ctrl, tuneGrid = grid)


grid = expand.grid(mtry = 1:6)
ctrl = trainControl(method='oob')

rf.bg = train(G4 ~ Alc + goout + freetime + romantic + activities + health, 
              data = studalc[-test,], 
              method = "rf", 
              trControl = ctrl, tuneGrid = grid)

grid = expand.grid(mtry = 1:8)
rf.bv = train(G4 ~ sex + address + Mjob + Fjob + nursery + Gedu + internet + traveltime, 
              data = studalc[-test,], 
              method = "rf", 
              trControl = ctrl, tuneGrid = grid)

g4.test <- function(mod) {
  pred = predict(mod, studalc[test,], type = "response")
  class.pred = ifelse(pred < 0.5, 0, 1)
  return(mean(studalc[test,"G4"] != class.pred))
}

G4.errors = numeric(8)
G4.errors[1] = g4.test(glm1)
G4.errors[2] = g4.test(glm2)
G4.errors[3] = g4.test(glm3)
G4.errors[4] = g4.test(glm4)
G4.errors[5] = mean(studalc[test,"G4"] != predict(knn.bv, studalc[test,]))
G4.errors[6] = mean(studalc[test,"G4"] != predict(knn.bg, studalc[test,]))
G4.errors[7] = mean(predict(rf.bv, studalc[test,]) != studalc[test,"G4"])
G4.errors[8] = mean(predict(rf.bg, studalc[test,]) != studalc[test,"G4"])

q1.models = c("glm.bv", "glm.bg", "glm.bv2", "glm.bg2", 
"knn.bv", "knn.bg", "rf.bv", "rf.bg")

fills=c("Logistic Regression","Logistic Regression","Logistic Regression","Logistic Regression","KNN","KNN","Random Forests","Random Forests")

ftable=data.frame(q1.models,G4.errors,fills)
ftable$q1.models=factor(ftable$q1.models,levels(ftable$q1.models)[c(3,1,5,8,7,6,4,2)])
ftable %>% 
ggplot() + geom_col(aes(x=q1.models,y=G4.errors,fill=factor(fills)))+ scale_fill_discrete(name = "Model Type", labels = c("KNN","Logistic Regression","Random Forest"))+
  ggtitle("Model Comparisons by Misclassification Rate")+
  geom_hline(aes(yintercept = 0.22), colour = "red", linetype = "dashed",size=1) + 
  guides(color=guide_legend(title="Model Type")) +
  ylim(0, 1) + 
  ylab("Test Misclassification Rate (%)") + 
  xlab("")
```


The bar chart above summarizes the test errors of the 8 models we trained. Models with “.bg” at the end represent models using the background predictors, and models with “.bv” are the models using the behavioral predictors. The “glm” label is, in our analysis, shorthand for binomial family GLMs—in other words, logistic regression models. The models glm.bg2 and glm.bv2 are the logistic regression models that fit on all two-way interactions between the predictors. “rf” and “knn” are abbreviations of random forest and KNN models. This bar chart highlights a peculiar feature of our models—5 models resulted in the exact same test error rate of 0.24. Investigating further, we looked at the specific types of predictions: false negatives (FN), false positives (FP), true negatives (TN), and true positives (TP).


```{r echo = FALSE, warning = FALSE}
tpfn <- function(pred) {
  df = data.frame(actual = studalc[test,"G4"], pred = pred) %>%
    mutate(type = ifelse(actual == 0 & pred == 1, "FP", 
                  ifelse(actual == 1 & pred == 0, "FN", 
                  ifelse(actual == 1 & pred == 1, "TP", "TN")))) %>%
    mutate(type = factor(type, levels = c("FP", "FN", "TP", "TN")))
  return(df)
}

glm.tpfn <- function(glm) {
  pred = predict(glm, studalc[test,], type = "response")
  class.pred = ifelse(pred < 0.5, 0, 1)
  return(tpfn(class.pred))
}

knn.tpfn <- function(knn) {
  pred = predict(knn, studalc[test,])
  return(tpfn(pred))
}

rf.tpfn <- function(rf) {
  pred = predict(rf, studalc[test,])
  return(tpfn(pred))
}

data.frame(glm.bv = glm.tpfn(glm1)$type, 
          glm.bg = glm.tpfn(glm2)$type, 
          glm.bv2 = glm.tpfn(glm3)$type, 
          glm.bg2 = glm.tpfn(glm4)$type, 
          knn.bg = knn.tpfn(knn.bg)$type, 
          knn.bv = knn.tpfn(knn.bv)$type,
          rf.bg = rf.tpfn(rf.bg)$type,
          rf.bv = rf.tpfn(rf.bv)$type) %>%
  ggplot() + 
  geom_point(aes(x = glm.bv, y = "glm.bv", colour = glm.bv), position = position_jitter(width = 0.3, height = 0.25)) + 
  geom_point(aes(x = glm.bg, y = "glm.bg", colour = glm.bg), position = position_jitter(width = 0.3, height = 0.25)) + 
  geom_point(aes(x = glm.bv2, y = "glm.bv2", colour = glm.bv2), position = position_jitter(width = 0.3, height = 0.25)) + 
  geom_point(aes(x = glm.bg2, y = "glm.bg2", colour = glm.bg2), position = position_jitter(width = 0.3, height = 0.25)) + 
  geom_point(aes(x = knn.bg, y = "knn.bg", colour = knn.bg), position = position_jitter(width = 0.3, height = 0.25)) + 
  geom_point(aes(x = knn.bv, y = "knn.bv", colour = knn.bv), position = position_jitter(width = 0.3, height = 0.25)) + 
  geom_point(aes(x = rf.bg, y = "rf.bg", colour = rf.bg), position = position_jitter(width = 0.3, height = 0.25)) + 
  geom_point(aes(x = rf.bv, y = "rf.bv", colour = rf.bv), position = position_jitter(width = 0.3, height = 0.25)) + 
  labs(colour = "Type") + ylab("") + xlab("") + 
  ggtitle("Test Classification Types per Model")
```


The plot above provides a summary of the classification types for our models. Here, we can see exactly what goes wrong with the models. The random forest models, simple logistic regression models, and one of the KNN models simply predict that all students will improve their grades from the first to second term—the 0.24 test error rate arises because there are 24 students with G4=0 in the test set. The only models that ever classify an observation as non-positive (either FN or TN) are glm.bg2, glm.bv2, and knn.bv. glm.bv2 and glm.bg2 overfit to the training data and thus give the highest test errors; this also explains why these two models are the only two to regularly classify observations as G4=0. Even the random forest models, which are able to handle categorical variables without turning them into dummy variables, do not give us good results—they still just classify everything as G4=1.


It seems the majority of our models are unable to find a clear pattern in either set of variables that lead to better accuracy than just predicting grade improvement (G4=1) for all students. We cannot determine which set of variables creates better models because neither set produces any useful classification model. Hence, neither the behavioral nor background variables in our data are useful predictors of grade improvement.




Our second analysis addresses the question: “which educational variables act as the best predictors for a student’s final grade?” To answer this, we trained several different regression models, each using a different subset of variables and their two-way interactions. Similar to the analysis for our first question, the models were first trained on the same 549 observation training set and compared on the basis of their performances on the 100 observation test set.


We first used the most basic linear model that simply uses each predictor in the regression; this model was named “full.” From this model, we applied two different variable selection methods: backward selection and best subsets. For the simple multiple regression model, both backward selection and best subsets resulted in the same model that we called “Best,” which uses schoolsup, studytime, failures, reason, firstgen as predictors. 


In addition, we considered two-way interactions and built models using a similar variable selection process. We first created Full2, which considers all predictors as well as all the two-way interaction terms. Then, like before, we applied the backward selection and best subsets methods; in this case, these resulted in two different models, which we labelled as “Back2” and “Best2.” We also trained lasso and ridge regression models using all the educational variables and their two-way interactions. We chose the optimal lasso and ridge penalty parameters with 10-fold cross-validation.


```{r echo = FALSE, warning = FALSE}
#Function to show results for regsubsets
# input should be the result from regsubsets
ShowSubsets=function(regout){
  z=summary(regout)
  q=as.data.frame(z$outmat)
  q$Rsq=round(z$rsq*100,2)
  q$adjRsq=round(z$adjr2*100,2)
  q$Cp=round(z$cp,2)
  return(q)
}



all = regsubsets(G3~schoolsup+famsup+paid+absences+studytime+failures+reason+firstgen, data=studalc[-test,], method="exhaustive")
#ShowSubsets(all)

#best model found with lowest CP
best.lm = lm(G3~schoolsup+studytime+failures+reason+firstgen, 
             data = studalc[-test,])

#summary(best.lm)

#intial linear model with all variables
full = lm(G3~schoolsup+famsup+paid+absences+studytime+
            failures+reason+firstgen, 
          data = studalc[-test,])

#use step() to find best predictors using backwards selection
#best.step = step(full, scale = (summary(full)$sigma)^2, trace = FALSE)
#summary(best.step)
#same as best subsets

all2 = regsubsets(G3~(schoolsup+famsup+paid+absences+studytime+failures+reason+firstgen)^2, data = studalc, method = "forward")
#ShowSubsets(all2)

#best model found using with loweset CP
best.lm2 = lm(G3~(studytime+failures+reason+schoolsup:studytime+schoolsup:failures+famsup:reason+absences:failures+studytime:reason), data = studalc[-test,])

#intial linear model with all variables
full2 = lm(G3~(schoolsup+famsup+paid+absences+studytime+failures+reason+firstgen)^2, data=studalc[-test,])

#use step() to find best predictors using backwards selection
best.step2 = step(full2, scale=(summary(full2)$sigma)^2, trace = FALSE, direction = "backward")

#summary(best.step2)

#lasso and ridge regression

#use model.matrix to format the categorical variables
#ensures that glmnet can actually fit a model
edu = model.matrix(~(schoolsup + famsup + paid + absences + studytime + failures + reason + firstgen)^2 - 1, studalc[-test,])

edu.test = model.matrix(~(schoolsup + famsup + paid + absences + studytime + failures + reason + firstgen)^2 - 1, studalc[test,])

G3 = model.matrix(~G3 - 1, studalc[-test,])


#use 10-fold CV to find best lambda for ridge and lasso
set.seed(5)
lasso = cv.glmnet(x = edu, y = G3, alpha = 1, nfolds = 10)

ridge = cv.glmnet(x = edu, y = G3, alpha = 0, nfolds = 10)


#get test RMSE for each model
G3.errors = numeric(7)
G3.errors[1] = sqrt(mean(studalc[test,"G3"] - 
                        predict(full, studalc[test,]))^2)
G3.errors[2] = sqrt(mean(studalc[test,"G3"] - 
                        predict(best.lm, studalc[test,]))^2)
G3.errors[3] = sqrt(mean(studalc[test,"G3"] - 
                        predict(full2, studalc[test,]))^2)
G3.errors[4] = sqrt(mean(studalc[test,"G3"] - 
                        predict(best.lm2, studalc[test,]))^2)
G3.errors[5] = sqrt(mean(studalc[test,"G3"] - 
                        predict(best.step2, studalc[test,]))^2)
G3.errors[6] = sqrt(mean(studalc[test,"G3"] - 
                        predict(lasso, edu.test))^2)
G3.errors[7] = sqrt(mean(studalc[test,"G3"] - 
                        predict(ridge, edu.test))^2)

q2.models = c("Full", "Best",
              "Full2", "Best2", "Back2", "Lasso", 
              "Ridge")

fills=c("Full","Best Subsets","Full","Best Subsets","Backward","Lasso","Ridge")

q2.errors = data.frame(model = q2.models, RMSE = G3.errors, fills) %>%
  mutate(model = factor(model, levels = c("Full2", "Full",
              "Back2", "Best", "Ridge", "Lasso", 
              "Best2")))

ggplot(q2.errors) + geom_col(aes(x = model, y = RMSE,fill=fills)) +
  scale_fill_discrete(name = "Model Technique", labels = c("Backward","Best Subsets","Full","Lasso","Ridge"))+
  xlab("Models") + ylab("Test RMSE") +coord_flip() + ggtitle("Model Comparisons by RMSE")

```


Once we trained all our models, we evaluated their performances on the test set using RMSE (root-mean-square error). Lasso and Best2 had the two lowest RMSE values, and their test performances were quite similar. We can see which of our educational variables are most predictive of G3 by examining the predictors of these two best models. The lasso regression model uses just 2 predictors: studytime and failures. Best2 uses several more predictors: studytime, failures, reason, studytime:schoolsup, failures:schoolsup, reason:famsup, studytime:reason, failures:absences. 

Though the Best2 model uses over triple the amount of predictors, the lasso regression model gives a very similar test performance, which leads us to suspect that those other variables are more “noise” than “signal.” We conclude that the two variables most predictive of G3 are studytime and failures. The plots below demonstrate the strength of the relationship between studytime and G3 as well as failures and G3.


```{r echo = FALSE}
g1 = ggplot(studalc) + 
  geom_point(aes(x = studytime, y = G3, colour = G3), 
             position = position_jitter(0.25, 0.25)) + 
  scale_color_gradient(low="blue", high="orange") + 
  theme(legend.position = "none")

g2 = ggplot(studalc) + 
  geom_point(aes(x = failures, y = G3, colour = G3), 
             position = position_jitter(0.25, 0.25)) + 
  scale_color_gradient(low="blue", high="orange") + ylab("") + 
  theme(legend.position = "none")

grid.arrange(g1, g2, ncol = 2)
```



# CONCLUSION

In our first question, we wanted to see which set of variables, behavioral or background, better predicts grade improvement from the first term to the second term by constructing several classification models; yet, our results suggest that grade improvement is not so deterministic. Contrary to our expectations, we found that neither set of variables ended up being useful in classifying grade improvement and that none of our models gave intelligible predictions. Our results show that such general information like parental information, health, and drinking behavior is mostly useless in trying to identify students who can improve their grades between terms. Drinker or non-drinker, male or female, romance or no romance, city or country—all such students have the same capacity to improve their grades. These results suggest that, rather than dismissing a student as a lost cause solely on the basis of his/her background or behavior, teachers should take a more holistic view of their students in order to determine whether a student has the ability to improve their grade.

Even so, we must not preclude the possibility that our data could just be missing some key behavioral or background information that can explain grade improvement, like household income and overall “busyness” during the grading period. This concept of time-specific “busyness” relates to the more general idea that grade change from one term to the next is most affected by circumstances specific to the grading term, such as illness, family hardship, relocation, an accident, or even a global pandemic. Perhaps this kind of student-specific variability results in a high degree of randomness that behavioral and background variables are simply unable to capture. Should such be the case, a model that accounts for subject-specific variability in its structure like a generalized linear mixed model (which allows for the inclusion of random effects) may give better results. We would recommend further analyses to strongly consider models including some kind of random effect.

In our second question, we wanted to find the educational variables most predictive of a student’s final grade. Through several different variable selection procedures, we created regression models each using different subsets of educational variables and their two-way interactions as predictors and compared their test performances as measured by RMSE. The two best models were Lasso and Best2; after examining their predictors, we found two variables that stood out: studytime and failures. We therefore concluded that studytime and failures had the highest impact on students’ final grade in our dataset. And these results make intuitive sense. The more students study, the better they will do in school. Failures, on the other hand, have the opposite effect; poor academic showing in the past is, of course, indicative of poor academic showing in the future. Keeping our results in mind, teachers and school faculty should be more wary of students with past history of poor academic performance; students that have failed one or more classes in the past likely need additional instruction just to stand level with their peers. However, academic success is in large part decided by the students themselves, particularly regarding their study habits. Teachers cannot make students study, yet we have shown that study time is absolutely essential to getting high grades. Hence, increasing grade averages is best achieved through a shifting of student culture and work ethic. However, that kind of change is much easier said than done. Efforts should be directed to promote responsibility on the part of the student and increase personal motivation to learn and study school material.

It is likely that more sophisticated regression procedures, such as KNN regression, random forest regression, and boosted tree regression, would have given us better test error, but these models all suffer from a lack of interpretability. Models like lasso regression allow us to pick out specific important variables and produce useful, easily understandable results. For future analysis of the same inquiry, we would recommend future studies to stick to basic regression procedures like linear and lasso regression but look at other variables that were not available to us in the dataset. A major drawback of our data is that almost all of the variables are categorical, owing to the fact that this data was collected through a survey/questionnaire of students. Perhaps more relevant variables, such as GPA and class rank, are as important, if not more so, than hours studied per week and number of past failures. Hence, we recommend that future analyses focus on more detailed, numeric data. Furthermore, the numeric variables present in the data were not detailed. For instance, studytime was given in tens of hours, and there were no options for 0 or any value between 1 and 10; we suspect that many students do not study at all or study less than 10 hours per week, yet this information is obscured by the overgeneralization of the studytime variable. Further investigation of this specific variable should collect data by allowing students to submit their own study time estimates instead of forcing them into broad sections.



